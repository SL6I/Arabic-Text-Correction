# -*- coding: utf-8 -*-
"""Scoring.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WYRLXGxfmoE-v5JjSj8EOTT4MaPtFSCz
"""

def read_lines(filepath):
    """Reads each line from a specified file and returns a list of stripped lines.

    Args:
        filepath (str): The path to the file to be read.

    Returns:
        list: A list of strings, where each string is a stripped line from the file.
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f.readlines()]

def evaluate_precision_recall_f1(raw_lines, cor_lines, pred_lines):
    """Calculates precision, recall, and F1 score by comparing tokens from raw, corrected, and predicted lines.

    Args:
        raw_lines (list of str): List of sentences with errors.
        cor_lines (list of str): List of corrected sentences serving as the gold standard.
        pred_lines (list of str): List of sentences corrected by the prediction model.

    Returns:
        tuple: Precision, recall, and F1 score as floats.
    """
    TP = 0  # True Positives: Correctly predicted corrections
    FP = 0  # False Positives: Incorrectly predicted corrections
    FN = 0  # False Negatives: Errors not corrected by the model

    for raw_line, cor_line, pred_line in zip(raw_lines, cor_lines, pred_lines):
        raw_tokens = raw_line.split()
        cor_tokens = cor_line.split()
        pred_tokens = pred_line.split()

        # Compare only up to the shortest sentence length to avoid indexing errors
        min_len = min(len(raw_tokens), len(cor_tokens), len(pred_tokens))
        for i in range(min_len):
            r_tok = raw_tokens[i]
            cor_tok = cor_tokens[i]
            pred_tok = pred_tokens[i]

            # Check if the prediction and correction are different from the raw token
            changed = (pred_tok != r_tok)
            gold_diff = (cor_tok != r_tok)

            # Evaluate each token correction attempt
            if changed and gold_diff:
                if pred_tok == cor_tok:
                    TP += 1  # Correct correction
                else:
                    FP += 1  # Incorrect correction
            elif changed and not gold_diff:
                FP += 1  # Unneeded correction
            elif not changed and gold_diff:
                FN += 1  # Missed correction

    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1

def main():
    """Runs the evaluation by loading data files and calculating evaluation metrics."""
    gold_file = r"C:\Users\SL6\Desktop\S\QALB-0.9.1-Dec03-2021-SharedTasks\data\2014\dev\QALB-2014-L1-Dev.cor"  # Gold standard corrected file
    raw_file = r"C:\Users\SL6\Desktop\S\QALB-0.9.1-Dec03-2021-SharedTasks\data\2014\dev\QALB-2014-L1-Dev.sent"  # File with original sentences containing errors
    pred_file = r"C:\Users\SL6\Desktop\S\QALB-0.9.1-Dec03-2021-SharedTasks\data\2014\dev\QALB-2014-L1-Dev.cor"  # Predictions made by the model

    cor_lines = read_lines(gold_file)
    raw_lines = read_lines(raw_file)
    pred_lines = read_lines(pred_file)

    precision, recall, f1 = evaluate_precision_recall_f1(raw_lines, cor_lines, pred_lines)
    print(f"Precision: {precision * 100:.2f}%")
    print(f"Recall: {recall * 100 :.2f}%")
    print(f"F1 Score: {f1 * 100:.2f}%")

if __name__ == "__main__":
    main()