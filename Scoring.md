## Evaluation Function for Precision, Recall, and F1  
Precision, Recall, and F1 Score are commonly used metrics information retrieval to evaluate the performance of models, especially in classification tasks.  

# Precision
**Definition**: Precision is the percentage of corrections made by the system that are actually correct.
![image alt](https://github.com/SL6I/Text-Correction/blob/b9782b0223ecc585a681c05df0b78988d7dab499/Precision.png)  
**Purpose**: Important when unnecessary corrections (false positives) could confuse or degrade text quality.

# Recall  
**Definition**: Recall is the percentage of actual errors in the text that the system successfully corrects.
![image.png](https://github.com/SL6I/Text-Correction/blob/015b61f6e89d34e42fbf829873b08f6b31657a73/Recall.png)      
**Purpose**: Important when missing errors (false negatives) leads to uncorrected mistakes.

# F1 Score  
**Definition**: F1 Score is the harmonic mean of precision and recall, providing a balanced measure when both are equally important.  

![image.png](https://github.com/SL6I/Text-Correction/blob/654b4a3c0b54540b6912d513f162b77d658187af/F1%20Score.png)  

**Purpose**: Useful in scenarios where both unnecessary corrections and missed corrections need to be minimized.  
