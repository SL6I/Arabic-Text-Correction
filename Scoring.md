## Evaluation Function for Precision, Recall, and F1  
Precision, Recall, and F1 Score are commonly used metrics information retrieval to evaluate the performance of models, especially in classification tasks.  

# Precision
**Definition**: Precision is the percentage of corrections made by the system that are actually correct.
![image alt](https://github.com/SL6I/Text-Correction/blob/b9782b0223ecc585a681c05df0b78988d7dab499/Precision.png)  
**Purpose**: Important when unnecessary corrections (false positives) could confuse or degrade text quality.

# Recall  
**Definition**: Recall is the percentage of actual errors in the text that the system successfully corrects.
![image.png](https://github.com/SL6I/Text-Correction/blob/b9782b0223ecc585a681c05df0b78988d7dab499/Precision.png)      
**Purpose**: Important when missing errors (false negatives) leads to uncorrected mistakes.
