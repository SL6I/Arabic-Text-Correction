# -*- coding: utf-8 -*-
"""Simple-BaseLine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11nu8Gp4PuJCf5TDuMbVex5Qkh6X9-nCv

# Import Libraries
"""

import collections

"""# Read Filesd As lines"""


# READ LINES FROM FILE

def read_lines(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f.readlines()]

"""# Simple BaseLine"""


# 2) BUILD A TOKEN-LEVEL DICTIONARY

def build_correction_dictionary(train_sent_path, train_cor_path):
    """
    Constructs a dictionary mapping raw tokens to their most frequent corrected tokens.

    Args:
        train_sent_path (str): Path to the raw/erroneous sentences file.
        train_cor_path (str): Path to the corrected sentences file.

    Returns:
        dict: Dictionary mapping raw_token -> most frequent corrected_token
    """
    # Read the raw and corrected lines
    raw_lines = read_lines(train_sent_path)
    cor_lines = read_lines(train_cor_path)

    # Initialize an empty dictionary to store correction frequencies
    freq_map = {}

    # Iterate over each pair of raw and corrected sentences
    for raw, cor in zip(raw_lines, cor_lines):
        # Split sentences into tokens
        raw_tokens = raw.split()
        cor_tokens = cor.split()

        # Compare tokens up to the length of the shorter sentence
        for rt, ct in zip(raw_tokens, cor_tokens):
            if rt != ct:
                if rt not in freq_map:
                    freq_map[rt] = {}
                if ct not in freq_map[rt]:
                    freq_map[rt][ct] = 0
                freq_map[rt][ct] += 1

    # Create the correction dictionary by selecting the most frequent correction for each raw token
    correction_dict = {rt: max(cts, key=cts.get) for rt, cts in freq_map.items()}

    return correction_dict


"""**Apply dictionar**"""


# APPLY THE DICTIONARY

def apply_correction_dictionary(sentence, correction_dict):
    """
    Splits a sentence into tokens and uses correction_dict to correct each token if known.
    Returns the corrected sentence (token-based) as a list of tokens.
    """
    tokens = sentence.split()
    corrected_tokens = []
    for tok in tokens:
        if tok in correction_dict:
            corrected_tokens.append(correction_dict[tok])
        else:
            corrected_tokens.append(tok)  
    return corrected_tokens

"""# Evaluation Function for Precision, Recall, and F1"""


# 4) EVALUATION: TOKEN-LEVEL PRECISION, RECALL, F1

def evaluate_precision_recall_f1(raw_lines, cor_lines, correction_dict):
    """
    Calculates precision, recall, and F1 score for token-level corrections.

        raw_lines (list of str): Raw or erroneous sentences.
        ref_lines (list of str): Corrected reference sentences.
        correction_dict (dict): A dictionary mapping incorrect tokens to corrected tokens.
"""

    TP = 0  # Count of correct changes
    FP = 0  # Count of incorrect changes
    FN = 0  # Count of missed necessary changes

    for raw_line, cor_line in zip(raw_lines, cor_lines):
        raw_tokens = raw_line.split()
        cor_tokens = cor_line.split()

        # Generate predictions based on the correction dictionary
        pred_tokens = apply_correction_dictionary(raw_line, correction_dict)

        # Only compare the minimum length of raw, reference, and predicted tokens
        min_len = min(len(raw_tokens), len(cor_tokens), len(pred_tokens))

        for i in range(min_len):
            r_tok = raw_tokens[i]
            cor_tok = cor_tokens[i]
            pred_tok = pred_tokens[i]

            # Check if the prediction changed the raw token
            changed = (pred_tok != r_tok)
            # Check if the correct differs from the raw token
            gold_diff = (cor_tok != r_tok)

            if changed and gold_diff:
                # Token was changed and it needed changing
                if pred_tok == cor_tok:
                    TP += 1
                else:
                    FP += 1
            elif changed and not gold_diff:
                # Token was changed but it didn't need changing
                FP += 1
            elif (not changed) and gold_diff:
                # Token was not changed but it needed changing
                FN += 1
            # If not changed and not gold_diff, it's a correct ignore (True Negative).

    precision = TP / (TP + FP) if (TP + FP) else 0.0
    recall = TP / (TP + FN) if (TP + FN) else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0

    return precision, recall, f1

"""# The Main"""

if __name__ == "__main__":

    train_sent = "/content/QALB-2014-L1-Train.sent"
    train_cor  = "/content/QALB-2014-L1-Train.cor"
    dev_sent   = "/content/QALB-2014-L1-Dev.sent"
    dev_cor    = "/content/QALB-2014-L1-Dev.cor"

    # STEP A: BUILD THE DICTIONARY FROM TRAIN
    correction_dict = build_correction_dictionary(train_sent, train_cor)
    print(f"Built dictionary with {len(correction_dict)} entries.")
    print(correction_dict)
    # STEP B: EVALUATE ON DEV
    dev_raw_lines = read_lines(dev_sent)
    dev_cor_lines = read_lines(dev_cor)

    precision, recall, f1 = evaluate_precision_recall_f1(dev_raw_lines, dev_cor_lines, correction_dict)

    print("=== Baseline Dictionary Model (DEV) ===")
    print(f"Precision: {precision:.3f}")
    print(f"Recall:    {recall:.3f}")
    print(f"F1:        {f1:.3f}")
